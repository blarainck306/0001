{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I apply random  forest (with parameter class_weight = 'balanced'). Let's see if we can get 'balanced' classification result, which means the percentage of recognized NDF users and non-NDF users are more or less the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import sklearn.grid_search\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from NDCG_score_func import ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading X,y and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= np.load('X.npy')\n",
    "y= np.load('y.npy')\n",
    "X_test= np.load('X_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a confusion matrix, we need a train data and test data and the true label and predicted value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first split the origitnal traiing data into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160088L, 226L)\n",
      "(53363L, 226L)\n",
      "(160088L,)\n",
      "(53363L,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  7, 11, ...,  7, 10,  7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  141   388   280   525  1245   613   687 31039   214    52 15635  2544]\n"
     ]
    }
   ],
   "source": [
    "print np.bincount(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit random forest classifier with training set and test the performance on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_features=40, n_estimators=120, n_jobs=1, min_samples_split=5,class_weight = 'balanced')#n_jobs=2 #, min_samples_split=5  max_features=12\n",
    "clf.fit(X_train,y_train)\n",
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the labels\n",
    "Labels = ['AU', 'CA', 'DE', 'ES', 'FR', 'GB', 'IT', 'NDF', 'NL', 'PT', 'US', 'other']\n",
    "confusion= pd.DataFrame(data = confusion_matrix(y_test, y_test_pred),index = Labels, columns=Labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   141.,    388.,    280.,    525.,   1245.,    613.,    687.,\n",
       "        31039.,    214.,     52.,  15635.,   2544.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test.astype(int)).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53363"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00454267  0.0125004   0.00902091  0.0169142   0.04011083  0.01974935\n",
      "  0.02213345  1.          0.00689455  0.00167531  0.50372113  0.0819614 ]\n"
     ]
    }
   ],
   "source": [
    "recip_freq = len(y_test) / 12 *np.bincount(y_test.astype(int)).astype(np.float64)\n",
    "print recip_freq/recip_freq[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Confusion Matrix__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following confusion matrix, the row index represent the true book type, and the columns index represents the predicted booking class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AU  CA  DE   ES   FR   GB   IT    NDF  NL  PT    US  other\n",
      "AU      0   0   0    0    1    0    0     50   1   0    87      2\n",
      "CA      0   0   1    4    5    0    2    137   0   0   230      9\n",
      "DE      0   0   0    0    1    0    0    116   0   0   157      6\n",
      "ES      0   0   1    1    6    0    2    210   1   0   297      7\n",
      "FR      0   2   2    2    8   10   10    478   1   0   717     15\n",
      "GB      0   2   2    3    3    5    1    228   1   1   359      8\n",
      "IT      1   1   1    2   10    2    3    289   1   0   358     19\n",
      "NDF    21  60  52  110  272  150  106  23051  39   3  6719    456\n",
      "NL      0   0   1    2    6    1    0     75   0   0   120      9\n",
      "PT      0   0   0    0    0    0    0     18   0   0    33      1\n",
      "US     11  21  28   48  114   42   65   5900  19   5  9145    237\n",
      "other   0   8   3   12   17    7    8   1056   1   1  1385     46\n"
     ]
    }
   ],
   "source": [
    "print confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22324 non-NDF users and 31039 NDF users in this synthetic test set.\n"
     ]
    }
   ],
   "source": [
    "total = confusion.sum(axis =1) #the total number of users for each class\n",
    "NDF = total[7]\n",
    "non_NDF = total.sum()-NDF\n",
    "print 'There are %d non-NDF users and %d NDF users in this synthetic test set.'%(non_NDF, NDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU        0.000000\n",
       "CA        0.000000\n",
       "DE        0.000000\n",
       "ES        0.190476\n",
       "FR        0.642570\n",
       "GB        0.815661\n",
       "IT        0.436681\n",
       "NDF      74.264635\n",
       "NL        0.000000\n",
       "PT        0.000000\n",
       "US       58.490566\n",
       "other     1.808176\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognized = pd.Series(np.diagonal(confusion), index = Labels)\n",
    "rec_perc = recognized/total *100\n",
    "rec_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Binarized reading__: In order to make the results more clear, we make a table containing the result of recoginized NDF and recognized non-NDF users. Notice that if a user actually booked Italy is recognized as booking France, it is counted as NOT recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reco_bina = pd.Series(data = [recognized.sum()-recognized['NDF'], recognized['NDF']],index = ['Non-NDF','NDF'])\n",
    "tot_bina = pd.Series(data = [total.sum()-total['NDF'], total['NDF']],index = ['Non-NDF','NDF'])\n",
    "rec_perc_bina = reco_bina/tot_bina *100\n",
    "# rec_perc_bina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of recognized non-NDF users is 41%, while the percentage of recoginized NDF users is 74%,\n"
     ]
    }
   ],
   "source": [
    "print 'The percentage of recognized non-NDF users is %d'%rec_perc_bina[0]+'%,'+ ' while the percentage of recoginized NDF users is %d'\\\n",
    "                                                          %rec_perc_bina[1]+'%,'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected balanced result is not obtained. This is partially caused  by the fact that if a user actually booked Italy is recognized as booking France, it is counted as NOT recognized. In order to get rid of this effect, let's apply 'balanced' Random forestto the whole data set with binarized labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy__ and __NDCG__ (Just of recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60451998575792221"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80414514213477373"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = y_test\n",
    "predictions = clf.predict_proba(X_test)\n",
    "ndcg_score(ground_truth, predictions, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
